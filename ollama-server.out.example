----------------------------------------------------------------------------------------

  Ollama Server Connection Details:

      Server: g002.clipper.gvsu.edu
        Port: 12754

  This job will end at 2025-11-12T07:41:53.
  Ollama server will gracefully terminate two minutes before the end of the job.

  To connect to this instance from outside of Clipper using SSH port forwarding:

  ssh -t -t hpcuser1@clipper.gvsu.edu -L 11434:localhost:12754 ssh g002.clipper.gvsu.edu -L 12754:localhost:12754

  This command will forward all requests on port 11434 from your local machine
  to the Ollama server instance running on g002.clipper.gvsu.edu.
  Please note this command will change for every submission of this job.
  Check this output for the correct ports to forward each time this job is submitted.

----------------------------------------------------------------------------------------

time=2025-11-07T07:41:54.488-05:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:12754 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/mnt/projects/hpcuser1_project/ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-07T07:41:54.489-05:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-11-07T07:41:54.489-05:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-07T07:41:54.490-05:00 level=INFO source=routes.go:1577 msg="Listening on 0.0.0.0:12754 (version 0.12.9)"
time=2025-11-07T07:41:54.491-05:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-07T07:41:54.493-05:00 level=INFO source=server.go:400 msg="starting runner" cmd="/mnt/home/hpcuser1/bin/ollama runner --ollama-engine --port 45743"
time=2025-11-07T07:41:54.600-05:00 level=INFO source=server.go:400 msg="starting runner" cmd="/mnt/home/hpcuser1/bin/ollama runner --ollama-engine --port 42719"
time=2025-11-07T07:41:54.675-05:00 level=INFO source=server.go:400 msg="starting runner" cmd="/mnt/home/hpcuser1/bin/ollama runner --ollama-engine --port 32887"
time=2025-11-07T07:41:54.774-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-1e747216-e5b0-3c3f-9258-7c70f375982f filtered_id="" library=CUDA compute=7.0 name=CUDA0 description="Tesla V100S-PCIE-32GB" libdirs=ollama,cuda_v12 driver=12.8 pci_id=0000:3b:00.0 type=discrete total="32.0 GiB" available="31.7 GiB"
